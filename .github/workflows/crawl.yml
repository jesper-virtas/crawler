name: batch-crawl-to-n8n

on:
  # 1) Manuell/extern start via API (från n8n)
  workflow_dispatch:
    inputs:
      sheet_url:
        description: "Google Sheet CSV (optional). If empty, uses repo urls.txt"
        required: false
      concurrency:
        description: "Parallel fetches"
        required: false
        default: "4"

concurrency:
  group: "batch-crawl"
  cancel-in-progress: true

jobs:
  run:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      # Cacha Playwright-browser för att spara Actions-minuter
      - name: Cache Playwright browsers
        id: cache-playwright
        uses: actions/cache@v4
        with:
          path: ~/.cache/ms-playwright
          key: ${{ runner.os }}-playwright-chromium-v1

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install crawl4ai
          python -m playwright install --with-deps chromium

      - name: Run batch crawl
        env:
          # Webhook till n8n (från repo secret)
          N8N_WEBHOOK_URL: ${{ secrets.N8N_WEBHOOK_URL }}

          # Om input saknas används secret SHEET_URL eller faller tillbaka till urls.txt
          SHEET_URL: ${{ inputs.sheet_url || secrets.SHEET_URL }}

          # Justerbar parallellism (från input, default 4)
          CONCURRENCY: ${{ inputs.concurrency }}
          URLS_FILE: urls.txt
          WAIT_MS: "6000"
          TIMEOUT: "45"
        run: |
          python fetch_crawl4ai.py
