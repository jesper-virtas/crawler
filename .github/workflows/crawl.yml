name: batch-crawl-to-n8n

on:
  workflow_dispatch:
    inputs:
      sheet_url:
        description: "Google Sheet CSV (optional). If empty, uses secret SHEET_URL or urls.txt"
        required: false
      concurrency:
        description: "Parallel fetches"
        required: false
        default: "4"


concurrency:
  group: "batch-crawl"
  cancel-in-progress: true

jobs:
  run:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      # Cachea pip (sparar nedladdade hjul/whls)
      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      # Cachea Playwright-browsers (största tidsbesparingen efter första körningen)
      - name: Cache Playwright browsers
        uses: actions/cache@v4
        with:
          path: ~/.cache/ms-playwright
          key: ${{ runner.os }}-playwright-chromium-v1
          restore-keys: |
            ${{ runner.os }}-playwright-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          python -m playwright install --with-deps chromium

      # Sätt SHEET_URL från inputs eller secrets (fallback till urls.txt om inget finns)
      - name: Resolve SHEET_URL
        run: |
          if [ -n "${{ inputs.sheet_url }}" ]; then
            echo "SHEET_URL=${{ inputs.sheet_url }}" >> $GITHUB_ENV
          elif [ -n "${{ secrets.SHEET_URL }}" ]; then
            echo "SHEET_URL=${{ secrets.SHEET_URL }}" >> $GITHUB_ENV
          else
            echo "No SHEET_URL provided (input/secret). Will use urls.txt."
          fi

      - name: Run batch crawl
        env:
          N8N_WEBHOOK_URL: ${{ secrets.N8N_WEBHOOK_URL }}
          SHEET_URL: ${{ env.SHEET_URL }}
          CONCURRENCY: ${{ inputs.concurrency }}
          URLS_FILE: urls.txt
          WAIT_MS: "6000"
          TIMEOUT: "45"
        run: |
          python fetch_crawl4ai.py
